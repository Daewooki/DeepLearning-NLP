{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자연어 처리를 위한 NLTK와 KoNLPy\n",
    "\n",
    "### 텍스트 전처리 (Text preprocessing) 을 위한 자연어 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *pip install nltk (Anaonda를 설치했다면 NLTK는 기본적으로 설치가 되어 있다.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK의 기능을 제대로 사용하기 위해서 NLTK Data를 추가적으로 설치한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실행하면 GUI 다운로드 관리자가 나오므로 Download 버튼을 눌러야 한다.\n",
    "다운로드가 완료되면 종료해야 셀이 종료된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 전처리 (Text preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 토큰화 (Tokenization)\n",
    "\n",
    "    웹 크롤링 등으로 얻어낸 코퍼스 데이터가 필요에 맞게 전처리 되지 않은 상태라면, 토큰화/정제/정규화를 해야한다. 주어진 코퍼스에서 토큰이라 불리는 단위로 나누는 작업을 토큰화라고 부른다. 보통 의미있는 단위로 토큰을 정의한다. \n",
    "    \n",
    "#### 1.1) 단어 토큰화 (Word Tokenization)\n",
    "    토큰의 기준을 단어로 하는 경우, 단어 토큰화라고 한다. 여기서 단어란, 단어 외에도 단어구, 의미를 갖는 문자열로 간주되기도 한다.\n",
    "    \n",
    "    예) 입력 : Life is beautiful. Time is too fast.\n",
    "        출력 : \"Life\", \"is\", \"beautiful\", \"Time\", \"is\", \"too\", \"fast\"\n",
    "        \n",
    "        간단하게, 구두점을 지운 뒤에 띄어쓰기를 기준으로 잘라내면 매우 간단하다. 하지만 구두점이나 특수문자를 전부 제거하면 토큰이 의미를 잃어버리는 경우가 발생하기도 한다. 한국어는 띄어쓰기만으로 토큰화를 하기에 어려운 점도 있다.\n",
    "        \n",
    "\n",
    "#### 1.2) 토큰화 중 생기는 선택의 순간\n",
    "    토큰화를 하다보면, 예상하지 못한 경우가 있어서 토큰화의 기준을 생각해봐야 하는 경우가 발생합니다. 물론, 이러한 선택은 해당 데이터를 가지고 어떤 용도로 사용할 것인지에 따라, 그 용도에 영향이 없는 기준으로 정하면 됩니다. 예를 들어 영어권 언어에서 아포스트로피를(')가 들어가있는 단어는 어떻게 토큰으로 분류해야할까라는 문제를 보여드리겠습니다.\n",
    "    \n",
    "    예를 들어봅시다.\n",
    "    Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\n",
    "\n",
    "    아포스트로피가 들어간 상황에서 Don't와 Jone's는 어떻게 토큰화할 수 있을까요?\n",
    "\n",
    "    * Don't\n",
    "    * Don t\n",
    "    * Dont\n",
    "    * Do n't\n",
    "    \n",
    "    원하는 결과가 나오도록 토큰화 도구를 직접 설계할 수 있지만, NLTK는 영어 코퍼스를 토큰화 하기 위한 도구를 제공한다. \n",
    "    \n",
    "  #### word_tokenize 와 WordPunctTokenizer를 사용해 NLTK의 (') 처리 방법을 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't를 Do와 n't 로 분리한 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "print(WordPunctTokenizer().tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordPunctTokenizer는 구두점을 별도로 분류하는 특징을 가지고 있기 때문에  word_tokenize와 달리 Don't를 Don과 '와 t로 분리하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras에서 text_to_word_sequence를 이용한 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence as ts\n",
    "print(ts(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras의 text_to_word_sequence 함수는 (')는 보존하며, 다른 구두점들은 제거한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
